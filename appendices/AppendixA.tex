\chapter{Log likelihood of the latent model~(\ref{eq:py})}\label{app:LL}
Assume that the number of learning samples tends to infinity, and so that in a small volume $dv = dxd{\bf y}$ around a given  joint configuration $(x,{\bf y})$,
the number of data $N_{x,{\bf y}}$ becomes large. Restricting the likelihood to this subset of the data yields the following:
\[
{\cal L}_{x,{\bf y}} = \prod_{m=1}^{N_{x,{\bf y}}} \sum_{\{\tau^{(m)}\}} 
\frac{\hat p(\tau^{(m)}\vert x)}{\prod_{i\in T}\sqrt{2\pi}\ \sigma_i(\tau^{(m)})}
\exp\Bigl(-\frac{1}{2}\sum_{i\in T}\frac{\bigl(y_i-\hat y_i(x)\bigr)^2}{\sigma_i(\tau^{(m)})^2}\Bigr).
\]
Upon introducing the relative frequencies:
\[
q_i(x,{\bf y}) = \frac{1}{N_{x,{\bf y}}}\sum_{m=1}^{N_{x,{\bf y}}} \tau_i^{(m)} 
\qquad\text{satisfying}\qquad 
\sum_{i\in T} q_i(x,{\bf y}) = 1,
\]
the sum over the $\tau_i^{(m)}$ is replaced by a sum over these new variables, with the summand obeying a large deviation principle\footnote{For sake of generality we appeal to the large deviation principle, but the same analysis can be made by direct summation over the latent variable here.} 
%(see e.g.~\cite{Touchette})
\[
{\cal L}_{x,{\bf y}} \asymp \sum_{\p} 
\exp\Bigl(-N_{x,{\bf y}} {\cal F}_{x,{\bf y}}\bigl[\p\bigr]\Bigr)
\]
where the rate function simply reads from Sanov's theorem
\[
{\cal F}_{x,{\bf y}}\bigl[\p\bigr] = \vert T\vert\log(\sigma)+
\sum_{i\in T}\Bigl[\bigl(y_i-\hat y_i(x)\bigr)^2\frac{1+\sum_{j\in T}\alpha_{ij}q_j}{2\sigma^2}
-\frac{1}{2}q_i\sum_{j\in T}\log(1+\alpha_{ji})+q_i\log\frac{q_i}{\hat p_i}\Bigr].
\]
Taking the saddle point for $q_i$ yield as a function of $(x,{\bf y})$ %expression~(\ref{eq:hatpi}). Inserting this into ${\cal F}$ and taking
expression~(7). Inserting this into ${\cal F}$ and taking
%the average over the data set yields the log likelihood~(\ref{eq:LL}) with
the average over the data set yields the log likelihood~(5) with 
opposite sign:
\[
{\cal L}({\bf x},{\bf y};\hat {\bf y},\hp,\sigma,\alpha) = -{\mathbb E}_{data}\Bigl[{\cal F}_{x,{\bf y}}\bigl[\p(x,{\bf y})\bigr]\Bigr].
\]
The optimization of the predictors $\hat {\bf y}$ yields:
\[
\frac{\partial{\cal L}}{\partial \hat y_i(x)} = \frac{1}{\sigma^2}{\mathbb E}_{data}\Bigl[ \bigl(y_i-\hat y_i(x)\bigr)\bigl(1+\sum_{j\in T}\alpha_{ij}q_j(x,{\bf y})\bigr)\Big\vert x\Bigr].
\]
The optimization of $\hp$ gives:
\[
\frac{\partial{\cal L}}{\partial \hat p_i(x)} = {\mathbb E}_{data}\Bigl[\frac{q_i(x,{\bf y})}{\hat p_i(x)}-\lambda(x)\Big\vert x \Bigr],
\]
with $\lambda(x)$ a Lagrange multiplier to insure that $\sum_i\hat p_i(x)=1$ 
%for any $x$ yielding expression~(\ref{eq:tildep}).
for any $x$ yielding expression~(9).

Finally the optimization $\alpha$ reads:
\[
\frac{\partial{\cal L}}{\partial \alpha_{ij}} = \frac{1}{2(1+\alpha_{ij})}{\mathbb E}_{data}\bigl[q_j(x,{\bf y})\bigr]-\frac{1}{2\sigma^2}
{\mathbb E}_{data}\bigl[\bigl(y_i-\hat y_i(x)\bigr)^2q_j(x,{\bf y})\bigr].
\]

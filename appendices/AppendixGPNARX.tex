\chapter{Notes On Gaussian Process Time Series Models}\label{app:gpNARX}

Gaussian processes provide a systematic and flexible framework for probabilistic inference in machine learning. 
Their formulation is general and their existence is dependent on two key conditions outlined in \cref{sec:osaGPmethod} 
which we restate here.

For any input space $\mathcal{X}$, a real valued scalar GP (\cref{eq:gpformulationapp}) can be created given two functions.

\begin{enumerate}
    \item A mean function $m: \mathcal{X} \longrightarrow \mathbb{R}$.
    \item A symmetric positive definite covariance function $K: \mathcal{X} \times \mathcal{X} \longrightarrow \mathbb{R}^{+}$ \citep[ch.~1\&2]{Berlinet2004}.
\end{enumerate}    

\begin{equation}\label{eq:gpformulationapp}
    f(x) \sim \mathcal{GP}(m(x), K(x, x'))
\end{equation}

It is important to note that there are no restrictions placed on the input space $\mathcal{X}$ in GPs. The inputs 
can be continuous, discrete or have complex structure. \citet[ch.~4, sec.~4.4]{Rasmussen:2005:GPM:1162254} give 
some examples of covariance functions defined over the space of strings (sequences of characters drawn from a 
finite alphabet) and note that it is indeed possible to construct covariance functions over structured objects 
such as trees and general graphs. 

Gaussian process time series modelling has a rich body of work \citep{turner2012gaussian,frigola2016bayesian}. There 
are two ways in which GP models can be applied to time series data: \begin{enumerate*} \item explicit time \& \item implicit time \end{enumerate*}

\section*{Explicit vs Implicit Representation}

Depending on whether or not time appears directly in the inputs space of a GP, we can classify a GP time series model as explicit or 
implicit respectively. Notationally, explicit and implicit time GP formulations are shown in \cref{eq:gpExplicit} 
and \cref{eq:gpImplicit} respectively.

\begin{align}
    y(t) &\sim \mathcal{GP}(m(t), K(t, s)) \label{eq:gpExplicit}\\
    y(t) &\sim \mathcal{GP}(m(\mathbf{x}(t)), K(\mathbf{x}(t), \mathbf{x}(s))) \label{eq:gpImplicit}
\end{align}

In the explicit time GP model, the mean and covariance of the process are functions of the continuous time 
coordinate $t$. On the other hand in implicit time GP models, the mean and covariance are functions of some 
state space $\mathbf{x}_t$. This key difference has significant implications for the system trajectories and 
uncertainty characteristics.

In section \S~\ref{sec:osa}, we formulated the $\mathrm{Dst}$ prediction model according to \cref{eq:gpImplicit}. 
By setting $\mathbf{x}(t)$ to a time history of $\mathrm{Dst}$ and possibly solar wind parameters, we built the 
GP-AR and GP-ARX prediction models.


\subsection*{GP-AR \& GP-ARX Models}

Implicit time GP models (\cref{eq:gpImplicit}) define probabilistic dynamics for a system $y(t)$ in terms
of some time varying system state $\mathbf{x}(t)$. To define their finite dimensional distributions using the 
GP methodology, we discretely subsample $y(t)$ and $\mathbf(t)$ and denote their discrete analogues as $y_t$ and 
$\mathbf{x}_t$ respectively.  

We give a detailed picture of the GP-AR model, its finite dimensional distribution, its sampling and dynamics. 
The GP-ARX model is a straight forward extension of the GP-AR dynamics shown below.

We first setup some background notation. The GP-AR system of order $p$, is determined by the system state 
$\mathbf{x}_t$; a dimensional vector composed of the time history of the system, shown in \cref{eq:systemStateAR}.

\begin{equation}\label{eq:systemStateAR}
    \mathbf{x}_t = \begin{bmatrix}
        y_{t-1}\\ 
        \vdots\\ 
        y_{t-p}\\ 
        \end{bmatrix}_{p \times 1}
\end{equation}

For every time step $t$, we combine all system states $\mathbf{x}_t, \cdots, \mathbf{x}_{p-1}$ into a design matrix 
$\mathbf{X}_t$ shown in \cref{eq:systemDesignMat}. The system trajectory $\mathbf{y}_t$ in \cref{eq:systemTraj} is the 
path taken by the system from time step $p$ to time step $t$.

\begin{equation}\label{eq:systemDesignMat}
    \mathbf{X}_t = \begin{bmatrix}
        \mathbf{x}^{\intercal}_{t}\\ 
        \vdots\\ 
        \mathbf{x}^{\intercal}_{p}\\ 
        \end{bmatrix}_{(t-p+1) \times p}
\end{equation}

\begin{equation}\label{eq:systemTraj}
    \mathbf{y}_t = \begin{bmatrix}
        y_{t}\\ 
        \vdots\\ 
        y_{p}\\ 
        \end{bmatrix}_{(t-p+1) \times 1}
\end{equation}

\subsubsection*{Joint Distribution}

The joint distribution of the system trajectory conditional on its initial state is a multivariate Gaussian 
as shown in \cref{eq:systemTrajProb}. The mean and covariance of the joint distribution are calculated using the 
mean function $m(.)$ and kernel $K(., .)$ from \cref{eq:gpImplicit}. 

Although the joint distribution of the system trajectory is straight forward to compute, sampling trajectories from 
the joint distribution is intractable because elements of the mean vector and covariance matrix are not computable 
unless the entire trajectory is known beforehand. 

To circumvent the sampling intractability of \cref{eq:systemTrajProb}, we use a probabilistic simulation based approach 
to the GP-AR process.

\begin{equation}\label{eq:systemTrajProb}
    y_{p}, \cdots, y_{t} \rvert y_{0}, \cdots, y_{p-1} \sim \mathcal{N}
    \left( 
        \begin{bmatrix}
            m(\mathbf{x}_{p})\\ 
            \vdots\\ 
            m(\mathbf{x}_{t})\\ 
        \end{bmatrix},
        \begin{bmatrix}
          K(\mathbf{x}_p, \mathbf{x}_p) & \cdots & K(\mathbf{x}_p, \mathbf{x}_t) \\
          \vdots & \ddots & \vdots \\
          K(\mathbf{x}_t, \mathbf{x}_p) & \cdots & K(\mathbf{x}_t, \mathbf{x}_t)\\
        \end{bmatrix}  
    \right) 
\end{equation}


\subsubsection*{GP-AR: Dynamics \& Sampling}

\begin{figure}[ht]
    \centering
    \noindent\includegraphics[width=0.75\textwidth]{gpar.png}
    \caption{Successive simulation of the GP-AR model}
    \label{fig:gparDiag}
\end{figure}


GP-AR and GP-ARX models can be understood as probabilistic simulation models. In \cref{fig:gparDiag} we can 
see a schematic illustration of such an iterative probabilistic system. To better understand \cref{fig:gparDiag}, 
we formalise the time stepping of the GP-AR model.

The iterative dynamics of GP-AR is captured in \cref{eq:gpARInit,eq:gpARProp}. The initial seed of the process is the 
system trajectory from time step $0$ to time step $p-1$ which is sampled from a standard multivariate Gaussian distribution. At each time step $t$, $y_t$ conditional to the system trajectory $\mathbf{y}_{t-1}$ is sampled 
from a Gaussian distribution (\cref{eq:gpARProp}) whose mean and covariance are calculated by appropriately 
conditioning the joint distribution in \cref{eq:systemTrajProb} and constructing the system state 
$\mathbf{x}_t$ and design matrix $\mathbf{X}_{t-1}$.     

\begin{align}
    (y_0, \cdots, y_{p-1}) &\sim \mathcal{N}(\mathbf{0}, \mathbf{I}) \label{eq:gpARInit}\\
    y_t \rvert \mathbf{y}_{t-1} &\sim \mathcal{N}(\bar{m}_t, \bar{\sigma}_{t}^2) \label{eq:gpARProp}
\end{align}

\begin{equation}\label{eq:postGPAR}
    \begin{aligned}
        \bar{m}_t &= m(\mathbf{x}_t) + 
        \mathbf{k}_{t}^{\intercal}\mathbf{K}_{t}^{-1}\left( \mathbf{y}_t - m(\mathbf{X}_{t-1}) \right) \\
        \bar{\sigma}^{2}_{t} &= K(\mathbf{x}_t, \mathbf{x}_t) - 
        \mathbf{k}_{t}^{\intercal} \mathbf{K}_{t}^{-1} \mathbf{k}_{t}
    \end{aligned}
\end{equation}

In \cref{eq:postGPAR}, the mean and variance of the conditional distribution of $y_t \rvert \mathbf{y}_{t-1}$ are 
computed, where $\mathbf{K}_{t} = [K(\mathbf{X}_{t-1}, \mathbf{X}_{t-1})]_{(t-p) \times (t-p)}$ is the 
covariance matrix computed between each pair of rows of $\mathbf{X}_{t-1}$ and 
$\mathbf{k}_{t} = [K(\mathbf{x}_t, \mathbf{X}_{t-1})]_{1 \times (t-p)}$ is the cross covariance matrix computed 
between the input features $\mathbf{x}_t$ and each row of the $\mathbf{X}_{t-1}$.

\subsection*{Relationship with Time Series Models : $\mathrm{AR}(p)$}

Due to the abstract nature of GPs, they actually generalize some popular time series models. Consider 
$\mathrm{AR}(p)$, the family of discrete auto-regressive time series models shown in \cref{eq:ARp}. At each 
time step, the systems state $y_t$ is determined by a linear combination of $p$ previous system states 
$y_{t-1}, \cdots, y_{t-p}$ added to Gaussian noise.  

\begin{align}
    y_t &= \sum^{p}_{k = 1}{\beta_{k}y_{t-k}} + z_t \label{eq:ARp} \\
    z_t &\sim \mathcal{N}(0, \sigma^{2}_{\varepsilon})
\end{align}

We can write \cref{eq:ARp} in probabilistic form as: 
\begin{equation*}
    y_t \rvert y_{t-1}, \cdots, y_{t-p} \sim \mathcal{N}(\sum^{p}_{k = 1}{\beta_{k}y_{t-k}}, \sigma^{2}_{\varepsilon})
\end{equation*}


In fact the the GP-AR model family contains the $\mathrm{AR}(p)$ model in \cref{eq:ARp} as a special case; by 
setting $m(\mathbf{x}_t) = \sum^{p}_{k = 1}{\beta_{k}Y_{t-k}}$ and 
$K(\mathbf{x}_t, \mathbf{x}_s) = \sigma^2_{\varepsilon}\delta(\mathbf{x}_t, \mathbf{x}_s)$, where $\delta(.,.)$ 
is the Dirac delta function, we obtain the conditional probability of $y_t \rvert y_{t-1}, \cdots, y_{t-p}$ as 
described above.

\section*{Example: System Identification}


\begin{equation}\label{eq:narp}
    \begin{aligned}
        y(t) &= -0.05 y_{t-1} + 0.25 y_{t-2} - 0.63 y_{t-3} - \\
        & 4\times10^{-3} y^2_{t-1} - 0.02 y^{2}_{t-3} - 0.05 y^{2}_{t-2} \\ 
        & - 0.048 y_{t-2} y_{t-1} - 0.02 y_{t-1} y_{t-3} - 0.06 y_{t-2} y_{t-3} + \varepsilon
    \end{aligned}
\end{equation}


\begin{figure}
    \centering
    \noindent\includegraphics[width=\textwidth]{nar_samples.pdf}
    \caption{Samples drawn from non-linear autoregressive model in \cref{eq:narp}}
    \label{fig:narpSamples}
\end{figure}




\begin{figure}
    \centering
    \noindent\includegraphics[width=\textwidth]{gp_nar_prior.pdf}
    \caption{Prior samples drawn from a GP-AR model (\cref{eq:gpARInit,eq:gpARProp}) with an squared exponential kernel.}
    \label{fig:gparPrior}
\end{figure}

\begin{figure}
    \centering
    \noindent\includegraphics[width=\textwidth]{gp_nar_pred_partial.pdf}
    \caption{Predictions made by the GP-AR model}
    \label{fig:gparPost}
\end{figure}

